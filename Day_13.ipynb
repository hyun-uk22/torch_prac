{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307c78db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12일차 코드\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root= \"data\",\n",
    "    train=True,\n",
    "    download =True,\n",
    "    transform = ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "class_names = train_data.classes\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True) # dataset to turn into iterable\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False) # don't necessarily have to shuffle the testing data\n",
    "\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units:int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels, 28*28형태의 이미지 -> [784]크기의 벡터)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layer_stack(x)\n",
    "    \n",
    "# input_shape = 784 : this is how many features goin in the model (in our case, it's one for every pixel in the target image)\n",
    "# hidden_units = 10 : number of units/neurons in the hidden layer(s)\n",
    "# output_shape = len(class_names) : Since working with a multi-class classification problem, we need an ouput neuron per class in our dataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(input_shape=784, \n",
    "                              hidden_units=10, \n",
    "                              output_shape=len(class_names)) # one for every class\n",
    "model_0.to(\"cpu\")\n",
    "\n",
    "\n",
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # loss function은 criterion, cost function이라고 불리기도 함\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start:float, end: float, device: torch.device = None):\n",
    "    total_time = end-start\n",
    "    print(f\"Train time on {device} : {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8b7a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'FashionMNISTModelV0', 'model_loss': 2.3190648555755615, 'model_acc': 10.852635782747603}\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and get Model 0 results\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               data_loader : torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn):\n",
    "    loss, acc = 0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in data_loader:\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y,\n",
    "                               y_pred = y_pred.argmax(dim=1))\n",
    "            \n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "\n",
    "    # 반환값 : 모델이 data_loader에 대해 예측한 결과를 담은 딕셔너리\n",
    "    return {\"model_name\": model.__class__.__name__,\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "model_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "                             loss_fn=loss_fn, accuracy_fn= accuracy_fn)\n",
    "print(model_0_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1555aca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# setup device agnostic-code\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6df194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1: Building a better model with non-linearity\n",
    "\n",
    "# non-linear and linear layers\n",
    "class FashionMNISTModelV1(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units:int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units),\n",
    "            nn.ReLU(), # adding non-linear layers\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1b6860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "model_1 = FashionMNISTModelV1(input_shape =784,\n",
    "                              hidden_units=10,\n",
    "                              output_shape=len(class_names)).to(device)\n",
    "print(next(model_1.parameters()).device)\n",
    "next(model_1.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b5c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss, optimizer and evaluation metrics\n",
    "\n",
    "from helper_functions import accuracy_fn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_1.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "166112ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functionizing training and test loops\n",
    "def train_step(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "        train_loss, train_acc = 0, 0\n",
    "        model.to(device)\n",
    "        for batch, (X,y) in enumerate(data_loader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "                y_pred = model(X)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                train_loss += loss\n",
    "                train_acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        train_loss /= len(data_loader)\n",
    "        train_acc /= len(data_loader)\n",
    "        print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device):\n",
    "        test_loss, test_acc = 0, 0\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "                for X,y in data_loader:\n",
    "                        X, y = X.to(device), y.to(device)\n",
    "\n",
    "                        test_pred = model(X)\n",
    "\n",
    "                        test_loss += loss_fn(test_pred, y)\n",
    "                        test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "                \n",
    "                test_loss /= len(data_loader)\n",
    "                test_acc /= len(data_loader)\n",
    "                print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec98853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hyun\\.conda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "--------------------\n",
      "Train loss: 1.09199 | Train accuracy: 61.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:12<00:25, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.95636 | Test accuracy: 65.00%\n",
      "\n",
      "Epoch: 1\n",
      "--------------------\n",
      "Train loss: 0.78101 | Train accuracy: 71.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:25<00:12, 12.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.72227 | Test accuracy: 73.91%\n",
      "\n",
      "Epoch: 2\n",
      "--------------------\n",
      "Train loss: 0.67027 | Train accuracy: 75.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:37<00:00, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.68500 | Test accuracy: 75.02%\n",
      "\n",
      "Train time on cuda : 37.642 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n--------------------\")\n",
    "    train_step(data_loader= train_dataloader,\n",
    "               model=model_1,\n",
    "               loss_fn=loss_fn,\n",
    "               optimizer=optimizer,\n",
    "               accuracy_fn=accuracy_fn)\n",
    "    test_step(data_loader=test_dataloader,\n",
    "              model=model_1,\n",
    "              loss_fn=loss_fn,\n",
    "              accuracy_fn=accuracy_fn)\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                            end=train_time_end_on_gpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f52142e",
   "metadata": {},
   "source": [
    "There's a small bottleneck(병목현상) between copying data from the CPU memory (default) to the GPU memory\n",
    "So for smaller models and datasets, the CPU might actuallly be the optimal place to compute on\n",
    "\n",
    "데이터셋과 모델이 작은 경우에 GPU로 데이터를 옮기는 데 드는 시간이 GPU의 속도 이점보다 더 커서 오히려 느려질 수 있음\n",
    "데이터셋과 모델이 클 경우, GPU의 연산 속도 이점이 전송 비용(the cost of getting the data there(GPU))보다 커짐\n",
    "\n",
    "# 병목 현상 : 시스템 전체 성능, 처리 속도, 효율 등을 제한하는 가장 느린 요소 또는 구간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eedb8ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Error cause of setup data and model to use device-agnostic code but not our evaluation function (함수에는 device-agnostic을 적용시키지 않았기 때문) \u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m model_1_results \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                             \u001b[49m\u001b[43maccuracy_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccuracy_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m model_1_results\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36meval_model\u001b[1;34m(model, data_loader, loss_fn, accuracy_fn)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X,y \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m---> 14\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[0;32m     16\u001b[0m         acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy_fn(y_true\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m     17\u001b[0m                            y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\hyun\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hyun\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mFashionMNISTModelV1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x:torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hyun\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hyun\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\hyun\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hyun\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hyun\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\hyun\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Error cause of setup data and model to use device-agnostic code but not our evaluation function (함수에는 device-agnostic을 적용시키지 않았기 때문) \n",
    "model_1_results = eval_model(model = model_1,\n",
    "                             data_loader = test_dataloader,\n",
    "                             loss_fn=loss_fn,\n",
    "                             accuracy_fn=accuracy_fn)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbc2eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'FashionMNISTModelV1', 'model_loss': 0.6850008368492126, 'model_acc': 75.01996805111821}\n",
      "{'model_name': 'FashionMNISTModelV0', 'model_loss': 2.3190648555755615, 'model_acc': 10.852635782747603}\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data.dataloader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module,\n",
    "               data_loader: torch.utils.data.dataloader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device):\n",
    "    loss, acc = 0,0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n",
    "        \n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "    return {\"model_name\": model.__class__.__name__,\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "model_1_results = eval_model(model=model_1, \n",
    "                             data_loader=test_dataloader,\n",
    "                             loss_fn = loss_fn, \n",
    "                             accuracy_fn=accuracy_fn,\n",
    "                             device=device)\n",
    "print(model_1_results)\n",
    "\n",
    "print(model_0_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782b232",
   "metadata": {},
   "source": [
    "Two of the main ways to fix overfitting (과적합 해결 주요방법 2가지)\n",
    "1. Using a smaller or different model (some models fit certain kinds of data better than others) (데이터 종류에 따라 더 잘 맞는 모델이 있음)\n",
    "2. Using a larger dataset (the more data, the more chance a model has to learn generalizable patterns) (데이터가 많을수록 모델이 일반화 가능한 패턴을 학습할 기회도 많아짐)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e5ef6",
   "metadata": {},
   "source": [
    "# Model 2: Building a Convolutional Neural Network(CNN)\n",
    "\n",
    "CNN's are known for their capabilities to find patterns in visual data\n",
    "\n",
    "Using CNN model : TinyVGG from the CNN Explainer website\n",
    "structure : Input layer -> [convolutional layer -> activation layer -> pooling layer] -> Output layer\n",
    " + [convolutional layer -> activation layer -> pooling layer]은 필요에 따라 확장하거나 반복 가능\n",
    "\n",
    "- 문제 유형에 따라 사용하는 모델 (예외도 존재함)\n",
    "1. Structured data(Excel spreadsheets, row and column data) | Model : Gradient boosted models, Random Forests, XGBoost | Code example : sklearn.ensemble, XGBoost library\n",
    "2. Unstructured data(images, audio, language) | Model : Convolutional Neural Networks, Transformers | Code example : torchvision.models, HuggingFace Transformers\n",
    "\n",
    "\n",
    "## CNN 모델\n",
    "1. 입력 이미지와 특징 추출\n",
    "\n",
    "2. 합성곱(Convolution) 연산\n",
    "  + Kernal : 작은 크기의 가중치 행렬, 이 커널이 입력 이미지 위를 슬라이딩하며 점진적으로 내적 연산(Dot product) 수행\n",
    "  + Dot product(내적 연산) : 커널 행렬과 입력 이미지의 해당 영역 픽셀값을 곱해서 모두 더함, 계산된 값이 feature map의 한 위치에 대응됨\n",
    "  + 특징 추출 : 커널은 엣지(선), 색상 변화, 패턴 등을 감지하도록 학습되어, 이미지 내 특정 특징을 잡아냄\n",
    "  + Stride(걸음걸이) : Kernal이 이미지 위를 이동하는 간격 (보통 1 또는 2 픽셀 단위로 움직이며, stride가 크면 출력 크기가 작아짐)\n",
    "  + Padding (패딩) : 입력 이미지 주변에 0 또는 특정 값을 덧붙여서 경계 부분에서 손실을 줄임\n",
    "\n",
    "3. Activation Function (활성화 함수) : Convoltion 결과에 바로 적용되는 함수, 음수 값을 0으로 만들고 양수는 그대로 둠 (비선형성 부여)\n",
    "\n",
    "4. Pooling layer : 일정 크기의 영역 안에서 가장 큰 값 선택\n",
    "  + 목적 : 공간 차원 축소(height, width 감소), 잡음과 작은 변동에 대한 강인성 증가, 연산 효율 향상\n",
    "\n",
    "5. 다중 레이어 구조 : convolution -> activation function -> pooling layer를 여러 층 쌓아올림\n",
    "\n",
    "6. Flatten layer(1차원 변환) : 다중 채널과 2D 공간으로 된 feature map을 1차원 벡터로 펼침 (해당 벡터가 이후의 Fully Connected Layer에 입력)\n",
    "\n",
    "7. Fully Connected Layer(완전연결층) : CNN의 마지막 단계는 보통 완전연결층, 모든 입력 뉴런과 출력 뉴런이 연결된 구조 (Flatten된 특징 벡터를 입력받아 클래스별 logits을 출력)\n",
    "\n",
    "8. Softmax function : logits를 확률 값으로 변환 (이 확률이 각 클래스에 속할 확률을 의미)\n",
    "\n",
    "9. Prediction : 가장 높은 확률을 가진 클래스를 최종 예측값으로 선택\n",
    "\n",
    "10. 하이퍼파라미터 조정 : Kernal size, Stride size, Padding size를 조절하여 출력 크기와 특징맵에 미치는 영향이 확인 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecf5b02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionMNISTModelV2(\n",
      "  (block_1): Sequential(\n",
      "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (block_2): Sequential(\n",
      "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# nn.Conv2d() and nn.MaxPool2d() layers from torch.nn\n",
    "from torch import nn\n",
    "\n",
    "class FashionMNISTModelV2(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.block_1 =nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,  # 3*3 크기의 필터(커널)를 이미지에 적용\n",
    "                      stride=1,   # 커널을 1씩 움직임(default, 기본값)\n",
    "                      padding=1),  # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                         stride=2)  # default stride value is same as kernel_size\n",
    "        )\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*7*7,   # 7*7은 합성곱 레이어 출력 크기 계산 공식에 따라서 나온 값\n",
    "                      out_features=output_shape)      # Where did this in_features shape come from? It's because each layer of our network compresses and changes the shape of our input data\n",
    "        )\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model_2 = FashionMNISTModelV2(input_shape=1,\n",
    "                              hidden_units=10,\n",
    "                              output_shape=len(class_names)).to(device)\n",
    "print(model_2)\n",
    "\n",
    "# What we've done is a common practice in machine learning. Find a model architecture somewhere and replicate with code\n",
    "\n",
    "# nn.Conv2d(), also known as a convolutional layer \n",
    "# nn.MaxPool2d(), also known as a max pooling layer\n",
    "\n",
    "# For other dimensional data (such as 1D for text or 3D for 3D objects) there's also nn.Conv1d() and nn.Conv3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41378dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 3, 64, 64]) -> [batch_size, color_channels, height, width]\n",
      "Single image shape: torch.Size([3, 64, 64]) -> [color_channels, height, width]\n",
      "Single image pixel values:\n",
      "tensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n",
      "         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n",
      "         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n",
      "         ...,\n",
      "         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n",
      "         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n",
      "         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n",
      "\n",
      "        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n",
      "         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n",
      "         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n",
      "         ...,\n",
      "         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n",
      "         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n",
      "         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n",
      "\n",
      "        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n",
      "         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n",
      "         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n",
      "         ...,\n",
      "         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n",
      "         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n",
      "         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "images = torch.randn(size=(32,3,64,64))\n",
    "test_image = images[0]\n",
    "print(f\"Image batch shape: {images.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Single image shape: {test_image.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Single image pixel values:\\n{test_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0214ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],\n",
      "         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],\n",
      "         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],\n",
      "         ...,\n",
      "         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],\n",
      "         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],\n",
      "         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],\n",
      "\n",
      "        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],\n",
      "         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],\n",
      "         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],\n",
      "         ...,\n",
      "         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],\n",
      "         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],\n",
      "         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],\n",
      "\n",
      "        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],\n",
      "         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],\n",
      "         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],\n",
      "         ...,\n",
      "         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],\n",
      "         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],\n",
      "         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],\n",
      "         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],\n",
      "         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],\n",
      "         ...,\n",
      "         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],\n",
      "         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],\n",
      "         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],\n",
      "\n",
      "        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],\n",
      "         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],\n",
      "         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],\n",
      "         ...,\n",
      "         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],\n",
      "         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],\n",
      "         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],\n",
      "\n",
      "        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],\n",
      "         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],\n",
      "         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],\n",
      "         ...,\n",
      "         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],\n",
      "         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],\n",
      "         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],\n",
      "       grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# in_channels (int) - Number of channels in the input image.\n",
    "# out_channels (int) - Number of channels produced by the convolution. (합성곱 연산을 통해 출력될 채널 수)\n",
    "# kernel_size (int or tuple) - Size of the convolving kernel/filter.\n",
    "# stride (int or tuple, optional) - How big of a step the convolving kernel takes at a time. Default: 1.\n",
    "# padding (int, tuple, str) - Padding added to all four sides of input. Default: 0. (입력의 네 가장자리에 추가되는 픽셀 수)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3,\n",
    "                       out_channels=10,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=0) # valid(패딩 없음), same(출력 크기를 입력과 같게 유지) 사용 가능\n",
    "\n",
    "# Pass the data through the convolutional layer\n",
    "print(conv_layer(test_image))\n",
    "\n",
    "# (If running PyTorch 1.11.0+, this won't occur)\n",
    "# nn.Conv2d() expects a 4d tensor as input \n",
    "# nn.Conv2d() layer expects a 4-dimensional tensor as input with size (N, C, H, W) or [batch_size, color_channels, height, width]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d0103f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n",
      "torch.Size([1, 10, 62, 62])\n"
     ]
    }
   ],
   "source": [
    "print(test_image.unsqueeze(dim=0).shape)\n",
    "print(conv_layer(test_image.unsqueeze(dim=0)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d47e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image \n",
    "                         out_channels=10,\n",
    "                         kernel_size=(5,5), # kernel is usually a square so a tuple also works (kernel size는 정사각형이 아니어도 상관없음)\n",
    "                         stride=2,\n",
    "                         padding=0)\n",
    "\n",
    "# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\n",
    "print(conv_layer_2(test_image.unsqueeze(dim=0)).shape)\n",
    "\n",
    "# Behind the scenes, nn.Conv2d() is compressing the information stored in the image\n",
    "# It does this by performing oprations on the input(test image) against its internal parameters\n",
    "# 지금까지 만들어온 신경망과 차이점은 레이어마다 파라미터 업데이트를 계산하는 방식이 다름, 각 레이어의 forward() 메서드에 정의된 연산 방식이 다름\n",
    "# (The only difference is how the different layers calculate their parameter updates or in Pytorch terms, the operation present in the layer forward() method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f41fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],\n",
      "          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],\n",
      "          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],\n",
      "          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],\n",
      "          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],\n",
      "\n",
      "         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],\n",
      "          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],\n",
      "          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],\n",
      "          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],\n",
      "          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],\n",
      "\n",
      "         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],\n",
      "          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],\n",
      "          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],\n",
      "          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],\n",
      "          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],\n",
      "\n",
      "\n",
      "        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],\n",
      "          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],\n",
      "          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],\n",
      "          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],\n",
      "          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],\n",
      "\n",
      "         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],\n",
      "          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],\n",
      "          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],\n",
      "          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],\n",
      "          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],\n",
      "\n",
      "         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],\n",
      "          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],\n",
      "          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],\n",
      "          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],\n",
      "          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],\n",
      "          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],\n",
      "          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],\n",
      "          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],\n",
      "          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],\n",
      "\n",
      "         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],\n",
      "          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],\n",
      "          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],\n",
      "          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],\n",
      "          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],\n",
      "\n",
      "         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],\n",
      "          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],\n",
      "          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],\n",
      "          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],\n",
      "          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],\n",
      "          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],\n",
      "          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],\n",
      "          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],\n",
      "          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],\n",
      "\n",
      "         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],\n",
      "          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],\n",
      "          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],\n",
      "          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],\n",
      "          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],\n",
      "\n",
      "         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],\n",
      "          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],\n",
      "          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],\n",
      "          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],\n",
      "          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],\n",
      "          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],\n",
      "          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],\n",
      "          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],\n",
      "          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],\n",
      "\n",
      "         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],\n",
      "          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],\n",
      "          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],\n",
      "          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],\n",
      "          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],\n",
      "\n",
      "         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],\n",
      "          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],\n",
      "          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],\n",
      "          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],\n",
      "          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],\n",
      "\n",
      "\n",
      "        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],\n",
      "          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],\n",
      "          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],\n",
      "          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],\n",
      "          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],\n",
      "\n",
      "         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],\n",
      "          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],\n",
      "          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],\n",
      "          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],\n",
      "          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],\n",
      "\n",
      "         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],\n",
      "          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],\n",
      "          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],\n",
      "          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],\n",
      "          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],\n",
      "\n",
      "\n",
      "        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],\n",
      "          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],\n",
      "          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],\n",
      "          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],\n",
      "          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],\n",
      "\n",
      "         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],\n",
      "          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],\n",
      "          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],\n",
      "          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],\n",
      "          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],\n",
      "\n",
      "         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],\n",
      "          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],\n",
      "          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],\n",
      "          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],\n",
      "          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],\n",
      "\n",
      "\n",
      "        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],\n",
      "          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],\n",
      "          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],\n",
      "          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],\n",
      "          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],\n",
      "\n",
      "         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],\n",
      "          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],\n",
      "          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],\n",
      "          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],\n",
      "          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],\n",
      "\n",
      "         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],\n",
      "          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],\n",
      "          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],\n",
      "          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],\n",
      "          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],\n",
      "\n",
      "\n",
      "        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],\n",
      "          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],\n",
      "          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],\n",
      "          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],\n",
      "          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],\n",
      "\n",
      "         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],\n",
      "          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],\n",
      "          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],\n",
      "          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],\n",
      "          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],\n",
      "\n",
      "         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],\n",
      "          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],\n",
      "          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],\n",
      "          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],\n",
      "          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],\n",
      "          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],\n",
      "          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],\n",
      "          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],\n",
      "          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],\n",
      "\n",
      "         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],\n",
      "          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],\n",
      "          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],\n",
      "          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],\n",
      "          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],\n",
      "\n",
      "         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],\n",
      "          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],\n",
      "          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],\n",
      "          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],\n",
      "          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,\n",
      "        -0.0797,  0.1121]))])\n"
     ]
    }
   ],
   "source": [
    "print(conv_layer_2.state_dict()) # check out the conv_layer_2 internal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de38b7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer_2 weight shape: \n",
      "torch.Size([10, 3, 5, 5]) -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n",
      "\n",
      "conv_layer_2 bias shape: \n",
      "torch.Size([10]) -> [out_channels =10]\n"
     ]
    }
   ],
   "source": [
    "print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -> [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\n",
    "print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -> [out_channels =10]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2329d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image original shape: torch.Size([3, 64, 64])\n",
      "Test image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\n",
      "Shape after going through conv_layer(): torch.Size([1, 10, 62, 62])\n",
      "Shape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])\n"
     ]
    }
   ],
   "source": [
    "# Stepping through nn.MaxPool2d()\n",
    "\n",
    "print(f\"Test image original shape: {test_image.shape}\")\n",
    "print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n",
    "\n",
    "max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\n",
    "print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n",
    "\n",
    "test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\n",
    "print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050252a",
   "metadata": {},
   "source": [
    "nn.Conv2d, nn.MaxPool2d 차이점\n",
    "\n",
    "1. nn.Conv2d <- 가중치와 편향을 학습, 커널이 입력 이미지의 각 영역과 가중치 기반 연산 수행\n",
    "2. nn.MaxPool2d <- 지정된 영역의 최댓값만 추출, 고정된 연산, 학습 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a955d325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
