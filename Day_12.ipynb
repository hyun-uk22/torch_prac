{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f76928af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root= \"data\",\n",
    "    train=True,\n",
    "    download =True,\n",
    "    transform = ToTensor(),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "class_names = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f6a471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x0000024626949D80>, <torch.utils.data.dataloader.DataLoader object at 0x0000024627B3F700>)\n",
      "Length of train dataloader: 1875 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "# prepare DataLoader\n",
    "# DataLoader helps load data into a model, it turns a large Dataset into a Python iterable of smaller chunks  (chunk : 큰 데이터를 작게 나눈 조각)\n",
    "# These smaller chunks are called batches or mini-batches and can be set by the batch_size parameter\n",
    "# When using really large datasets, unless you've got infinite computing power, it's easeier to break them up into batches. It also gives your model more opportunities to improve\n",
    "# With mini-batches(small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch)\n",
    "# What's good batch size? -> 32 is a good place to start for a fair amount of problems.\n",
    "# But since this is a hyperparameter you can try all different kinds of values, though generally powers of 2(2의 거듭제곱 값) are use most often\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle=True) # dataset to turn into iterable\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False) # don't necessarily have to shuffle the testing data\n",
    "\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81539aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "print(train_features_batch.shape, train_labels_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: torch.Size([1, 28, 28])\n",
      "Label: 7, label size: torch.Size([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "random_idx = torch.randint(0, len(train_features_batch),size=[1]).item()  # size를 리스트로 받는 이유는 다차원 텐서를 만들 수 있도록 되어있기 때문\n",
    "img, label = train_features_batch[random_idx], train_labels_batch[random_idx]\n",
    "plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.title(class_names[label])\n",
    "plt.axis(\"Off\")\n",
    "print(f\"Image size: {img.shape}\")\n",
    "print(f\"Label: {label}, label size: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3d33eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([32, 1, 28, 28]) -> [color_channels, height, width]\n",
      "Shape after flattening: torch.Size([32, 784]) -> [color_channels, height*width]\n",
      "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5020, 0.0196, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.1373, 0.0549, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Model 0 : Build a baseline model\n",
    "\n",
    "# nn.Flatten() : compresses the dimensions of a tensor into a single vector (텐서의 여러 차원을 하나의 벡터로 압축해줌)\n",
    "flatten_model = nn.Flatten()\n",
    "\n",
    "x = train_features_batch\n",
    "output = flatten_model(x)\n",
    "\n",
    "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
    "\n",
    "print(x)\n",
    "print(output)\n",
    "\n",
    "# nn.Flatten() layer took out shape from [color_channels, height, width] to [color_channels, height*width]\n",
    "# Turned the pixel data from height and width dimensions into one long feature vector. And nn.Linear() layer like their inputs to be in the form of feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292a69d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FashionMNISTModelV0(\n",
       "  (layer_stack): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=True)\n",
       "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "class FashionMNISTModelV0(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units:int, output_shape:int):\n",
    "        super().__init__()\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Flatten(), # neural networks like their inputs in vector form\n",
    "            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels, 28*28형태의 이미지 -> [784]크기의 벡터)\n",
    "            nn.Linear(in_features=hidden_units, out_features=output_shape)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layer_stack(x)\n",
    "    \n",
    "# input_shape = 784 : this is how many features goin in the model (in our case, it's one for every pixel in the target image)\n",
    "# hidden_units = 10 : number of units/neurons in the hidden layer(s)\n",
    "# output_shape = len(class_names) : Since working with a multi-class classification problem, we need an ouput neuron per class in our dataset\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model_0 = FashionMNISTModelV0(input_shape=784, \n",
    "                              hidden_units=10, \n",
    "                              output_shape=len(class_names)) # one for every class\n",
    "model_0.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f73f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exists, skipping download\n",
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "# setup loss, optimizer and evaluation metrics\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "    print(\"Downloading helper_functions.py\")\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")  # Need the \"raw\" GitHub URL for this to work\n",
    "    print(type(request))\n",
    "    with open(\"helper_functions.py\",\"wb\") as f:\n",
    "        f.write(request.content) # .text로 저장할 수도 있지만 .content가 어떤 종류의 파일이든 안전하게 저장 가능하므로 사용(원본 바이트 그대로 가져오기 때문)\n",
    "\n",
    "# requests.get : 웹 주소로 GET 요청을 보내, .py파일의 소스 코드 내용을 텍스트로 반환받음 (request는 Response 객체가 됨)\n",
    "# f.write : 파일에 다운로드한 코드 저장\n",
    "# request.content : 요청 받은 데이터의 바이너리 형태(bytes)\n",
    "request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")  # Need the \"raw\" GitHub URL for this to work\n",
    "print(type(request))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b5c3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # loss function은 criterion, cost function이라고 불리기도 함\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7defc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to time our experiments  # timeit.default_timer()\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fdf787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "--------------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:10<00:21, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.42655 | Test loss: 0.45735, Test acc: 84.02%\n",
      "\n",
      "Epoch: 1\n",
      "--------------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:22<00:11, 11.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.42340 | Test loss: 0.45961, Test acc: 84.20%\n",
      "\n",
      "Epoch: 2\n",
      "--------------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:33<00:00, 11.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train loss: 0.41963 | Test loss: 0.46264, Test acc: 83.99%\n",
      "\n",
      "Train time on cpu: 33.456 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a training loop and training a model on batches of data\n",
    "\n",
    "# Since computing on batches of data, loss and evaluation metrics will be calculated per batch rather than across the whole dataset (손실 함수나 평가 지표는 배치 단위로 계산됨)\n",
    "# This means we'll have to divide our loss and accuracy values by the number of batches in each dataset's respective dataloader (손실과 정확도 값을 각 데이터셋의 dataloader의 수로 나눠야함)\n",
    "# 1. Loop through epochs \n",
    "# 2. Loop through training batches, perform training steps, calculate the train loss per batch\n",
    "# 3. Loop through testing batches, perform testing steps, calculate the test loss per batch\n",
    "# 4. Print out what's happening\n",
    "# 5. Time it all(for fun)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_time_start_on_cpu = timer() # 훈련 시작 시점 시간 저장\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):  # progress bar 표기 (루프 진행 상황)\n",
    "    print(f\"Epoch: {epoch}\\n--------------\")\n",
    "    train_loss = 0\n",
    "    for batch, (X,y) in enumerate(train_dataloader):\n",
    "        model_0.train()\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # calculate loss (per batch)  (각 배치에 대한 손실을 누적해서 train loss에 더함)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch%400 ==0:\n",
    "            print(f\"Looked at {batch*len(X)}/{len(train_dataloader.dataset)} samples\")  # len(X)는 입력 데이터(32크기의 batch)이므로 32가 나옴\n",
    "            \n",
    "    train_loss /= len(train_dataloader)   # train_loss를 batch 수로 나눠서 평균 손실로 만듦    # len(train_dataloader)는 1875임, 60000개의 데이터셋을 32 크기의 batch로 묶은 개수\n",
    "\n",
    "    test_loss, test_acc = 0,0\n",
    "\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X,y in test_dataloader:\n",
    "            test_pred = model_0(X)\n",
    "            test_loss += loss_fn(test_pred,y)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        # 배치 수로 나눠 평균 손실, 평균 정확도 계산\n",
    "        test_loss /= len(test_dataloader) \n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,\n",
    "                                            end=train_time_end_on_cpu,\n",
    "                                            device =str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50bc7bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90288463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
